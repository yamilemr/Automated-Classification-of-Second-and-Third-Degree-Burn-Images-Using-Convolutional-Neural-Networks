{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sequential Hyperparameter Search and Sensitivity Analysis of Color Channels**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements sequential hyperparameter search, sensitivity analysis of the RGB color channel, and systematic experiment tracking using Weights & Biases (wandb).\n",
    "\n",
    "Instead of performing a full grid search (computationally expensive), a sequential strategy is used:  \n",
    "At each stage:\n",
    "- Only one hyperparameter is varied\n",
    "- All other hyperparameters remain fixed\n",
    "- The best-performing value is selected\n",
    "- That value is fixed before moving to the next stage\n",
    "\n",
    "This approach significantly reduces computational cost while maintaining controlled experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "\n",
    "from tools import load_images_with_labels, calculate_metrics, evaluate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image dimensions\n",
    "width = 540\n",
    "height = 960\n",
    "\n",
    "#Path where images are stored and organized by class\n",
    "path = '../data/burn_images/'\n",
    "\n",
    "#Channel selection for sensitivity analysis\n",
    "#['red'], ['green'], ['blue']\n",
    "#['red','green'], ['red','blue'], ['green','blue']\n",
    "#['red','green','blue'] or 'bgr'\n",
    "channels = ['green', 'blue'] \n",
    "n_channels = len(channels) #Number of input channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load images with their corresponding labels using only selected channels\n",
    "X, y = load_images_with_labels(path=path, channels=channels)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset into 80% training and 20% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of X_val:', X_val.shape)\n",
    "print('Shape of y_train:', y_train.shape)\n",
    "print('Shape of y_val:', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función para cambiar los hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función recibe un hiperparámetro y una lista de hiperparámetros y se van probando uno por uno en el modelo.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diccionario con los valores base de los hiperparámetros\n",
    "base_hyperparams = {\n",
    "    'conv_layers': 3,\n",
    "    'filters_layer_1': 32,\n",
    "    'filters_layer_2': 32,\n",
    "    'filters_layer_3': 64,\n",
    "    'kernel_size': 3,\n",
    "    'strides': 2,\n",
    "    'dense_layers': 1,\n",
    "    'dense_units_1': 64,\n",
    "    'dense_units_2': 64,\n",
    "    'dense_units_3': 64,\n",
    "    'dropout_rate': 0.4,\n",
    "    'batch_size': 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cambiar_parametro(diccionario, hiperparametro, lista_hiperparametros):\n",
    "    '''\n",
    "    Función que cambia el valor de un hiperparámetro en un diccionario y genera un nombre de ejecución.\n",
    "    Es decir, modifica un único hiperparámetro mientras mantiene los demás constantes.\n",
    "\n",
    "    Parámetros:\n",
    "    - diccionario (dict): Diccionario que contiene los parámetros del modelo.\n",
    "    - hiperparametro (str): Clave del parámetro que se desea cambiar.\n",
    "    - lista_hiperparametros (list): Lista de valores que se asignarán al parámetro.\n",
    "\n",
    "    Yields:\n",
    "    - tuple: Un nombre de ejecución (str) y el diccionario actualizado (dict).\n",
    "    Si el parámetro no se encuentra en el diccionario, imprime un mensaje de error y retorna None.\n",
    "    '''\n",
    "    for valor in lista_hiperparametros:\n",
    "        #Se modifica únicamente el hiperparámetro objetivo\n",
    "        if hiperparametro in diccionario:\n",
    "            diccionario[hiperparametro] = valor\n",
    "        else:\n",
    "            print(f'El hiperparametro {hiperparametro} no se encuentra en el diccionario')\n",
    "            return None\n",
    "        \n",
    "        #Construcción del nombre del experimento\n",
    "        nombre_run = (f\"cl:{diccionario['conv_layers']}, \" \n",
    "        + f\"fl1:{diccionario['filters_layer_1']}, \")\n",
    "        \n",
    "        if diccionario['conv_layers'] >= 2: \n",
    "            nombre_run = nombre_run + f\"fl2:{diccionario['filters_layer_2']}, \"\n",
    "        if diccionario['conv_layers'] >= 3: \n",
    "            nombre_run = nombre_run + f\"fl3:{diccionario['filters_layer_3']}, \"\n",
    "\n",
    "        nombre_run = (nombre_run + f\"ks:{diccionario['kernel_size']}, \"\n",
    "            + f\"st:{diccionario['strides']}, \"\n",
    "            + f\"dl:{diccionario['dense_layers']}, \"\n",
    "            + f\"du1:{diccionario['dense_units_1']}, \")\n",
    "        \n",
    "        if diccionario['dense_layers'] >= 2:\n",
    "            nombre_run = nombre_run + f\"du2:{diccionario['dense_units_2']}, \"  \n",
    "        if diccionario['dense_layers'] >= 3:\n",
    "            nombre_run = nombre_run + f\"du3:{diccionario['dense_units_3']}, \"  \n",
    "\n",
    "        nombre_run = (nombre_run + f\"dr:{diccionario['dropout_rate']}, \"\n",
    "            + f\"bs:{diccionario['batch_size']}\")\n",
    "\n",
    "        yield nombre_run, diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear generador de nombres y diccionarios\n",
    "generador_parametros = cambiar_parametro(base_hyperparams, 'batch_size', [32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A partir de aquí correr para probar los demás valores de la lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_run, parametros = next(generador_parametros)\n",
    "print(nombre_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First convolutional block (includes input shape)\n",
    "arquitectura = [layers.Conv2D(parametros[\"filters_layer_1\"], \n",
    "                              kernel_size=(parametros[\"kernel_size\"], parametros[\"kernel_size\"]), \n",
    "                              strides=(parametros[\"strides\"], parametros[\"strides\"]),\n",
    "                              input_shape=(height, width, n_channels)),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.Activation('relu'),\n",
    "                layers.MaxPooling2D(pool_size=(2, 2))]\n",
    "\n",
    "#Remaining convolutional layers (added according to the value of conv_layers)\n",
    "if parametros['conv_layers'] > 1:\n",
    "    arquitectura += [layers.Conv2D(parametros[\"filters_layer_2\"], \n",
    "                                   kernel_size=(parametros[\"kernel_size\"], parametros[\"kernel_size\"]),\n",
    "                                   strides=(parametros[\"strides\"], parametros[\"strides\"])),\n",
    "                    layers.BatchNormalization(),\n",
    "                    layers.Activation('relu'),\n",
    "                    layers.MaxPooling2D(pool_size=(2, 2))]\n",
    "\n",
    "    if parametros['conv_layers'] > 2:\n",
    "        arquitectura += [layers.Conv2D(parametros[\"filters_layer_3\"], \n",
    "                                       kernel_size=(parametros[\"kernel_size\"], parametros[\"kernel_size\"]),\n",
    "                                       strides=(parametros[\"strides\"], parametros[\"strides\"])),\n",
    "                        layers.BatchNormalization(),\n",
    "                        layers.Activation('relu'),\n",
    "                        layers.MaxPooling2D(pool_size=(2, 2))]\n",
    "            \n",
    "#Feature aggregation\n",
    "arquitectura += [layers.GlobalAveragePooling2D()]\n",
    "\n",
    "#Fully connected layers (added according to the value of dense_layers)\n",
    "arquitectura += [layers.Dense(parametros[\"dense_units_1\"]),\n",
    "                layers.Activation('relu'),\n",
    "                layers.Dropout(parametros[\"dropout_rate\"])]\n",
    "\n",
    "if parametros['dense_layers'] > 1:\n",
    "    arquitectura += [layers.Dense(parametros[\"dense_units_2\"]),\n",
    "                    layers.Activation('relu'),\n",
    "                    layers.Dropout(parametros[\"dropout_rate\"])]\n",
    "\n",
    "    if parametros['dense_layers'] > 2:\n",
    "        arquitectura += [layers.Dense(parametros[\"dense_units_3\"]),\n",
    "                        layers.Activation('relu'),\n",
    "                        layers.Dropout(parametros[\"dropout_rate\"])]\n",
    "\n",
    "#Output layer: probability of third-degree burn\n",
    "arquitectura += [layers.Dense(1),\n",
    "                layers.Activation('sigmoid')]\n",
    "\n",
    "model = tf.keras.Sequential(arquitectura) #Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the final model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model compilation\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear experimento en wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea el experimento en wandb\n",
    "configuracion = {'conv_layers': parametros[\"conv_layers\"],\n",
    "                 'filters_layer_1': parametros[\"filters_layer_1\"],\n",
    "                 'kernel_size': parametros[\"kernel_size\"],\n",
    "                 'strides': parametros[\"strides\"],\n",
    "                 'dense_layers': parametros[\"dense_layers\"],\n",
    "                 'dense_units_1': parametros[\"dense_units_1\"],\n",
    "                 'dropout_rate': parametros[\"dropout_rate\"],\n",
    "                 'batch_size': parametros[\"batch_size\"]}\n",
    "\n",
    "if parametros[\"conv_layers\"] > 1:\n",
    "    configuracion['filters_layer_2'] = parametros[\"filters_layer_2\"]\n",
    "    if parametros[\"conv_layers\"] > 2:\n",
    "        configuracion['filters_layer_3'] = parametros[\"filters_layer_3\"]\n",
    "\n",
    "if parametros[\"dense_layers\"] > 1:\n",
    "    configuracion['dense_units_2'] = parametros[\"dense_units_2\"]\n",
    "    if parametros[\"dense_layers\"] > 2:\n",
    "        configuracion['dense_units_3'] = parametros[\"dense_units_3\"]\n",
    "\n",
    "run = wandb.init(project=\"CNN_quemaduras_2\", \n",
    "                 entity=\"frantorres14\",\n",
    "                 name=\"_\".join(channels), #Poner nombre_run para la búsqueda de hiperparámetros\n",
    "                 config=configuracion)\n",
    "\n",
    "config = wandb.config\n",
    "wandb_logger = WandbMetricsLogger(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se entrena el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop training when the model stops improving\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10, #Wait 10 epochs without improvement before stopping\n",
    "    restore_best_weights=True #Restore weights from the epoch with the best val_loss\n",
    ")\n",
    "\n",
    "#Reduce the learning rate when the model stagnates\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.5, #Reduce the learning rate by half when there is no improvement\n",
    "    patience=5, #Wait 5 epochs without improvement before reducing\n",
    "    min_lr=1e-7 #Minimum allowed learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=config.batch_size,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[WandbMetricsLogger(), early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM_train, accuracy_train, precision_train, recall_train, f1_train = calculate_metrics(model, X_train, y_train)\n",
    "evaluate_model(model, X_train, y_train, dataset='Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM_val, accuracy_val, precision_val, recall_val, f1_val = calculate_metrics(model, X_val, y_val)\n",
    "evaluate_model(model, X_val, y_val, dataset='Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar métricas en wandb\n",
    "wandb.log({\"accuracy_train\": accuracy_train,\n",
    "           \"precision_train\": precision_train,\n",
    "           \"recall_train\": recall_train,\n",
    "           \"f1_train\": f1_train,\n",
    "           \"accuracy_val\": accuracy_val,\n",
    "           \"precision_val\": precision_val,\n",
    "           \"recall_val\": recall_val,\n",
    "           \"f1_val\": f1_val})\n",
    "\n",
    "# Termina el experimento\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetros a probar\n",
    "\n",
    "'conv_layers': [1, 2, 3]  \n",
    "'filters_layer_k': [16, 32, 64]  \n",
    "'kernel_size': [3, 5]  \n",
    "'strides': [1, 2, 3]  \n",
    "'dense_layers': [1, 2, 3]  \n",
    "'dense_units_k': [32, 64, 128]  \n",
    "'dropout_rate': [0.3, 0.4, 0.5]  \n",
    "'batch_size': [16, 32, 64]  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
