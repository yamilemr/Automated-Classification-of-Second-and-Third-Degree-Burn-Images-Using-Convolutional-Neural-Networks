{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sequential Hyperparameter Search and Sensitivity Analysis of Color Channels**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements sequential hyperparameter search, sensitivity analysis of the RGB color channel, and systematic experiment tracking using Weights & Biases (wandb).\n",
    "\n",
    "Instead of performing a full grid search (computationally expensive), a sequential strategy is used:  \n",
    "At each stage:\n",
    "- Only one hyperparameter is varied\n",
    "- All other hyperparameters remain fixed\n",
    "- The best-performing value is selected\n",
    "- That value is fixed before moving to the next stage\n",
    "\n",
    "This approach significantly reduces computational cost while maintaining controlled experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "\n",
    "from tools import load_images_with_labels, calculate_metrics, evaluate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image dimensions\n",
    "width = 540\n",
    "height = 960\n",
    "\n",
    "#Path where images are stored and organized by class\n",
    "path = '../data/burn_images/'\n",
    "\n",
    "#Channel selection for sensitivity analysis\n",
    "channels = ['green', 'blue'] \n",
    "n_channels = len(channels) #Number of input channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load images with their corresponding labels using only selected channels\n",
    "X, y = load_images_with_labels(path=path, channels=channels)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset into 80% training and 20% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of X_val:', X_val.shape)\n",
    "print('Shape of y_train:', y_train.shape)\n",
    "print('Shape of y_val:', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to change hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function receives a hyperparameter and a list of hyperparameters, which are tested one by one in the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary containing the base hyperparameter values\n",
    "base_hyperparams = {\n",
    "    'conv_layers': 3,\n",
    "    'filters_layer_1': 32,\n",
    "    'filters_layer_2': 32,\n",
    "    'filters_layer_3': 64,\n",
    "    'kernel_size': 3,\n",
    "    'strides': 2,\n",
    "    'dense_layers': 1,\n",
    "    'dense_units_1': 64,\n",
    "    'dense_units_2': 64,\n",
    "    'dense_units_3': 64,\n",
    "    'dropout_rate': 0.4,\n",
    "    'batch_size': 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_hyperparam(config_dict, hyperparam, hyperparams_list):\n",
    "    '''\n",
    "    Function that changes the value of a hyperparameter in a dictionary and generates a run name.\n",
    "    That is, it modifies a single hyperparameter while keeping the others constant.\n",
    "\n",
    "    Parameters:\n",
    "    - config_dict (dict): Dictionary containing the model parameters.\n",
    "    - hyperparam (str): Key of the parameter to be changed.\n",
    "    - hyperparams_list (list): List of values to assign to the parameter.\n",
    "\n",
    "    Yields:\n",
    "    - tuple: A run name (str) and the updated dictionary (dict).\n",
    "    If the parameter is not found in the dictionary, it prints an error message and returns None.\n",
    "    '''\n",
    "    for value in hyperparams_list:\n",
    "        #Modify only the target hyperparameter\n",
    "        if hyperparam in config_dict:\n",
    "            config_dict[hyperparam] = value\n",
    "        else:\n",
    "            print(f'Hyperparameter {hyperparam} not found in dictionary')\n",
    "            return None\n",
    "        \n",
    "        #Build experiment run name\n",
    "        run_name = (f\"cl:{config_dict['conv_layers']}, \" \n",
    "                    f\"fl1:{config_dict['filters_layer_1']}, \")\n",
    "        \n",
    "        if config_dict['conv_layers'] >= 2: \n",
    "            run_name += f\"fl2:{config_dict['filters_layer_2']}, \"\n",
    "        if config_dict['conv_layers'] >= 3: \n",
    "            run_name += f\"fl3:{config_dict['filters_layer_3']}, \"\n",
    "\n",
    "        run_name += (f\"ks:{config_dict['kernel_size']}, \"\n",
    "                     f\"st:{config_dict['strides']}, \"\n",
    "                     f\"dl:{config_dict['dense_layers']}, \"\n",
    "                     f\"du1:{config_dict['dense_units_1']}, \")\n",
    "        \n",
    "        if config_dict['dense_layers'] >= 2:\n",
    "            run_name += f\"du2:{config_dict['dense_units_2']}, \"  \n",
    "        if config_dict['dense_layers'] >= 3:\n",
    "            run_name += f\"du3:{config_dict['dense_units_3']}, \"  \n",
    "\n",
    "        run_name += (f\"dr:{config_dict['dropout_rate']}, \"\n",
    "                     f\"bs:{config_dict['batch_size']}\")\n",
    "\n",
    "        yield run_name, config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create generator of run names and configurations\n",
    "hyperparam_generator = change_hyperparam(base_hyperparams, 'batch_size', [32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here, execute to evaluate the remaining values in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve the next hyperparameter configuration from the generator\n",
    "run_name, hyperparams = next(hyperparam_generator)\n",
    "print(run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture is dynamically built based on the selected hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First convolutional block (includes input shape)\n",
    "model_architecture = [layers.Conv2D(filters=hyperparams['filters_layer_1'], \n",
    "                                    kernel_size=(hyperparams['kernel_size'], hyperparams['kernel_size']), \n",
    "                                    strides=(hyperparams['strides'], hyperparams['strides']),\n",
    "                                    input_shape=(height, width, n_channels)),\n",
    "                      layers.BatchNormalization(),\n",
    "                      layers.Activation('relu'),\n",
    "                      layers.MaxPooling2D(pool_size=(2, 2))]\n",
    "\n",
    "#Remaining convolutional layers (added according to the value of conv_layers)\n",
    "if hyperparams['conv_layers'] > 1:\n",
    "    model_architecture += [layers.Conv2D(filters=hyperparams['filters_layer_2'], \n",
    "                                         kernel_size=(hyperparams['kernel_size'], hyperparams['kernel_size']),\n",
    "                                         strides=(hyperparams['strides'], hyperparams['strides'])),\n",
    "                           layers.BatchNormalization(),\n",
    "                           layers.Activation('relu'),\n",
    "                           layers.MaxPooling2D(pool_size=(2, 2))]\n",
    "\n",
    "    if hyperparams['conv_layers'] > 2:\n",
    "        model_architecture += [layers.Conv2D(filters=hyperparams['filters_layer_3'], \n",
    "                                             kernel_size=(hyperparams['kernel_size'], hyperparams['kernel_size']),\n",
    "                                             strides=(hyperparams['strides'], hyperparams['strides'])),\n",
    "                               layers.BatchNormalization(),\n",
    "                               layers.Activation('relu'),\n",
    "                               layers.MaxPooling2D(pool_size=(2, 2))]\n",
    "            \n",
    "#Feature aggregation\n",
    "model_architecture += [layers.GlobalAveragePooling2D()]\n",
    "\n",
    "#Fully connected layers (added according to the value of dense_layers)\n",
    "model_architecture += [layers.Dense(units=hyperparams['dense_units_1']),\n",
    "                       layers.Activation('relu'),\n",
    "                       layers.Dropout(rate=hyperparams['dropout_rate'])]\n",
    "\n",
    "if hyperparams['dense_layers'] > 1:\n",
    "    model_architecture += [layers.Dense(units=hyperparams['dense_units_2']),\n",
    "                           layers.Activation('relu'),\n",
    "                           layers.Dropout(rate=hyperparams['dropout_rate'])]\n",
    "\n",
    "    if hyperparams['dense_layers'] > 2:\n",
    "        model_architecture += [layers.Dense(units=hyperparams['dense_units_3']),\n",
    "                               layers.Activation('relu'),\n",
    "                               layers.Dropout(rate=hyperparams['dropout_rate'])]\n",
    "\n",
    "#Output layer: probability of third-degree burn\n",
    "model_architecture += [layers.Dense(units=1),\n",
    "                       layers.Activation('sigmoid')]\n",
    "\n",
    "#Instantiate the CNN model with the defined architecture\n",
    "model = tf.keras.Sequential(model_architecture) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the final model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model compilation\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create experiment in wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build configuration dictionary for experiment tracking\n",
    "configuracion = {'conv_layers': hyperparams['conv_layers'],\n",
    "                 'filters_layer_1': hyperparams['filters_layer_1'],\n",
    "                 'kernel_size': hyperparams['kernel_size'],\n",
    "                 'strides': hyperparams['strides'],\n",
    "                 'dense_layers': hyperparams['dense_layers'],\n",
    "                 'dense_units_1': hyperparams['dense_units_1'],\n",
    "                 'dropout_rate': hyperparams['dropout_rate'],\n",
    "                 'batch_size': hyperparams['batch_size']}\n",
    "\n",
    "if hyperparams['conv_layers'] > 1:\n",
    "    configuracion['filters_layer_2'] = hyperparams['filters_layer_2']\n",
    "    if hyperparams['conv_layers'] > 2:\n",
    "        configuracion['filters_layer_3'] = hyperparams['filters_layer_3']\n",
    "\n",
    "if hyperparams['dense_layers'] > 1:\n",
    "    configuracion['dense_units_2'] = hyperparams['dense_units_2']\n",
    "    if hyperparams['dense_layers'] > 2:\n",
    "        configuracion['dense_units_3'] = hyperparams['dense_units_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize W&B run\n",
    "run = wandb.init(project='CNN_quemaduras_2', \n",
    "                 entity='frantorres14',\n",
    "                 name='_'.join(channels), #run_name for hyperparameter search\n",
    "                 config=configuracion)\n",
    "\n",
    "config = wandb.config #Access w&b configuration object\n",
    "wandb_logger = WandbMetricsLogger(config) #Callback to automatically log training metrics per epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop training when the model stops improving\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10, #Wait 10 epochs without improvement before stopping\n",
    "    restore_best_weights=True #Restore weights from the epoch with the best val_loss\n",
    ")\n",
    "\n",
    "#Reduce the learning rate when the model stagnates\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.5, #Reduce the learning rate by half when there is no improvement\n",
    "    patience=5, #Wait 5 epochs without improvement before reducing\n",
    "    min_lr=1e-7 #Minimum allowed learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=config.batch_size,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[WandbMetricsLogger(), early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation metrics on the training set\n",
    "CM_train, accuracy_train, precision_train, recall_train, f1_train = calculate_metrics(model, X_train, y_train)\n",
    "evaluate_model(model, X_train, y_train, dataset='Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation metrics on the validation set\n",
    "CM_val, accuracy_val, precision_val, recall_val, f1_val = calculate_metrics(model, X_val, y_val)\n",
    "evaluate_model(model, X_val, y_val, dataset='Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Record final performance metrics in wandb\n",
    "wandb.log({'accuracy_train': accuracy_train,\n",
    "           'precision_train': precision_train,\n",
    "           'recall_train': recall_train,\n",
    "           'f1_train': f1_train,\n",
    "           'accuracy_val': accuracy_val,\n",
    "           'precision_val': precision_val,\n",
    "           'recall_val': recall_val,\n",
    "           'f1_val': f1_val})\n",
    "\n",
    "#Finish the experiment\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter search space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'conv_layers': [1, 2, 3]  \n",
    "'filters_layer_k': [16, 32, 64]  \n",
    "'kernel_size': [3, 5]  \n",
    "'strides': [1, 2, 3]  \n",
    "'dense_layers': [1, 2, 3]  \n",
    "'dense_units_k': [32, 64, 128]  \n",
    "'dropout_rate': [0.3, 0.4, 0.5]  \n",
    "'batch_size': [16, 32, 64]  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
